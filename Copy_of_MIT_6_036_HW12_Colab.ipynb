{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of MIT 6.036 HW12 Colab",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yangyuegly/6.036HW/blob/new_branch/Copy_of_MIT_6_036_HW12_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNJnLrgd6aLV",
        "colab_type": "text"
      },
      "source": [
        "#MIT 6.036 Spring 2019: Homework 12"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tj65sGzr8O7e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BL-awiRyGp84",
        "colab_type": "text"
      },
      "source": [
        "#Setup\n",
        "Run the next code block to download and import the code for this lab.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsHM3W44G2iJ",
        "colab_type": "code",
        "outputId": "9dc35689-9572-4f7b-f112-7121b7b7d7a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!wget --quiet https://introml.odl.mit.edu/cat-soop/_static/6.036/homework/hw12/data_for_hw12.zip\n",
        "!unzip data_for_hw12.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  data_for_hw12.zip\n",
            "  inflating: movies.csv              \n",
            "  inflating: ratings.csv             \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "724STOOC9skZ",
        "colab_type": "text"
      },
      "source": [
        "#Some preliminary code\n",
        "\n",
        "Here are some useful functions which will be used in the following sections:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjLcTfzu936E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pred(data, x):\n",
        "    (a, i, r) = data\n",
        "    (u, b_u, v, b_v) = x\n",
        "    return np.dot(u[a].T,v[i]) + b_u[a] + b_v[i]\n",
        "  \n",
        "# X : n x k\n",
        "# Y : n\n",
        "def ridge_analytic(X, Y, lam):\n",
        "    (n, k) = X.shape\n",
        "    xm = np.mean(X, axis = 0, keepdims = True)   # 1 x n\n",
        "    ym = np.mean(Y)                              # 1 x 1\n",
        "    Z = X - xm                                   # d x n\n",
        "    T = Y - ym                                   # 1 x n\n",
        "    th = np.linalg.solve(np.dot(Z.T, Z) + lam * np.identity(k), np.dot(Z.T, T))\n",
        "    # th_0 account for the centering\n",
        "    th_0 = (ym - np.dot(xm, th))                 # 1 x 1\n",
        "    return th.reshape((k,1)), float(th_0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFTBSXas8Tdu",
        "colab_type": "text"
      },
      "source": [
        "## 2) Some movies are more equal than others\n",
        "\n",
        "In Lab 12, we formulated recommender systems without offsets:  there was no equivalent of $\\theta_0$ in the regression problem we solved.  But offsets are very useful in this problem to account for some reviewers that are just generally grumpy (or enthusiastic) or some movies that are just generally awful (or great). For this section we will work to extend the results from the lab to include offsets.\n",
        "\n",
        "Rather than trying to predict what rating Amy will give a movie based on the properties of that movie, it might be more effective to predict how much \"more highly than usual\" Amy will rate the movie.  To do this, we introduce a vector $b_u$ to characterize offsets for users and a vector $b_v$ to characterize offsets for movies.  Now, the objective can be written as\n",
        "$$J(U, V) = \\frac{1}{2}\\sum_{(a, i) \\in D} (Y_{ai} - {u^{(a)}}\\cdot v^{(i)} - b_u^{(a)} - b_v^{(i)})^2 + \\frac{\\lambda}{2} \\sum_{a = 1}^n \\lVert u^{(a)} \\rVert^2 + \\frac{\\lambda}{2} \\sum_{i = 1}^m  \\lVert v^{(i)} \\rVert^2 $$\n",
        "\n",
        "We will stick with just one half of the problem for now:  finding $U$ and $b_u$ given a fixed $V$ and $b_v$.  We'll concentrate on computing a new estimate for $u^{(a)}$ and $b_u^{(a)}$.  Continuing the notation from lab, the regression problem that we have to solve becomes:\n",
        "$$-B_a^T (Z_a - B_a u^{(a)} - b_v - b_u^{(a)}) + \\lambda u^{(a)} = 0$$\n",
        "where $b_v$ is the $l_a$ by 1 vector of offsets for the movies and $b_u^{(a)}$ is the offset for user $a$.  This is a linear regression problem in which the targets can be viewed as $Z_a - b_v$, and where there is an offset $b_u^{(a)}$ that is not regularized.  This is, then, the standard ridge regression set-up.  We have seen how to solve this problem via gradient descent before, but it can also be solved analytically by essentially turning it into a regular linear regression problem. Using this approach to computing the optimal $u^{(a)}$ requires first centering the data, then doing linear regression, then doing a little bit of work to recover the offset $b_u^{(a)}$.  In the code, we have supplied a procedure (`ridge_analytic`) for doing exactly this.\n",
        "\n",
        "Let's see how it works out in our running example.  For illustrative purposes, we'll add two more movies:\n",
        "\n",
        "8) 6.036 lecture videos <br />\n",
        "9) The Zzzz files <br />\n",
        "\n",
        "Our new preference data is:\n",
        "$$Y = \\begin{bmatrix}\n",
        "? & 1 & ? & 1 & 5 & 1 & 5 & 5 & 1\\\\\n",
        "1 & 5 & 1 & ? & ? & 5 & 1 & 5 & 1\\\\\n",
        "5 & 5 & 5 & 5 & ? & ? & 5 & ? & 1\\\\\n",
        "1 & ? & 1 & 1 & 1 & 1 & ? & 5 & ?\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "We will assume the following value for $b_v$:\n",
        "$$b_v = \\begin{bmatrix}\n",
        "3 & 3 & 3 & 3 & 3 & 3 & 3 & 5 & 1\n",
        "\\end{bmatrix}^T $$\n",
        "And let\n",
        "$$V = \\begin{bmatrix}\n",
        "10 & 1 & 10 & 1 & 10 & 1 & 10 & 5 & 5\\\\\n",
        "1 & 10 & 1 & 10 & 1 & 10 & 1 & 5 & 5\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "We can now compute the optimal $u^{(a)}$ and $b_u^{(a)}$ for $a=0$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4c7Wb4F9QxM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Z = np.array([[1], [1], [5], [1], [5], [5], [1]])\n",
        "b_v = np.array([[3], [3], [3], [3], [3], [5], [1]])\n",
        "B = np.array([[1, 10], [1, 10], [10, 1], [1, 10], [10, 1], [5, 5], [5, 5]])\n",
        "# Solution with offsets, using ridge_analytic provided in code file\n",
        "u_a, b_u_a = ridge_analytic(B, (Z - b_v), 1)\n",
        "\n",
        "#u_a, b_u_a\n",
        "#(array([[ 0.22024566],\n",
        "#        [-0.22193986]]),\n",
        "# array([[ 0.00762389]]))\n",
        "\n",
        "# Solution using previous model, with no offsets\n",
        "u_a_no_b = np.dot(np.linalg.inv(np.dot(B.T, B) + 1 * np.identity(2)), np.dot(B.T, Z))\n",
        "\n",
        "#u_a_no_b\n",
        "#array([[ 0.50148126],\n",
        "#       [ 0.0562376 ]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bN6IqdGQ-QT6",
        "colab_type": "text"
      },
      "source": [
        "Based on this, we can predict Amy's ratings on new Movies!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XUz5dlFJUgS",
        "colab_type": "text"
      },
      "source": [
        "1) How will Amy feel about a brand new Llama movie (not in the existing movie data) that gets bad ratings from almost everyone ($b_v^{(i)}= 1$), in the two models?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gkpCpPiJi3B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Code for answering (1) here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cur4TZYQJVKW",
        "colab_type": "text"
      },
      "source": [
        "2) What about a brand new Robot movie (not in the existing movie data) that gets good ratings from almost everyone ($b_v^{(i)}= 5$), in the two models?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZ4iIytKJlrK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Code for answering (2) here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTXAET0IJVzC",
        "colab_type": "text"
      },
      "source": [
        "3) What about a brand new Robot movie (not in the existing movie data) that gets average ratings from almost everyone ($b_v^{(i)}= 3$), in the two models?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O30gG-qq-Z4O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Code for answering (3) here\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keW70T5N-iJj",
        "colab_type": "text"
      },
      "source": [
        "##3) Implementing recommender systems\n",
        "\n",
        "Now we'll look in detail at two implementations of matrix factorization.  One is the alternating least squares algorithm, and the other is the stochastic gradient descent algorithm.\n",
        "\n",
        "We will assume that the input data is made up of `(a, i, r)` triples, where `a` is a user index. `i` is an item index and `r` is a rating.  Here is a small example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmC7xNyD_GcG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data is a list of (a, i, r) triples\n",
        "ratings_small = \\\n",
        "[(0, 0, 5), (0, 1, 3), (0, 3, 1),\n",
        " (1, 0, 4), (1, 3, 1),\n",
        " (2, 0, 1), (2, 1, 1), (2, 3, 5),\n",
        " (3, 0, 1), (3, 3, 4),\n",
        " (4, 1, 1), (4, 2, 5), (4, 3, 4)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoDdcZmp_O00",
        "colab_type": "text"
      },
      "source": [
        "We will assume that predictions are made using user and item offsets, that is,\n",
        "$$y = {u^{(a)}}\\cdot v^{(i)} + b_u^{(a)} + b_v^{(i)}$$\n",
        "In this expression $b_u$ is a vector of user offsets and $b_v$ is a vector of item offsets.\n",
        "\n",
        "### 3.1) Alternating Least Squares (ALS)\n",
        "Below is a function that provides the \"outer loop\" of the alternating least squares algorithm.\n",
        "\n",
        "* Define n and m from the data.\n",
        "* Initialize a list of lists `us_from_v` where `us_from_v[i]` contains the indices and ratings of users who rated item `i`.\n",
        "* Similarly, initialize a list of lists `vs_from_u` where `vs_from_u[a]` contains the indices and ratings of items rated by user `a`.\n",
        "* Initialize the set of parameters `x` (note that the u,v entries are set randomly while the user and item offsets are set to 0) where \n",
        "    * `x[0] = u`, a list of column vectors (initialized randomly) such that `u[a]` corresponds to $u^{(a)}$ as defined above.\n",
        "    * `x[1] = b_u`, a column vector (initialized with 0s) equal to $b_u$ as defined above.\n",
        "    * `x[2] = v`, a list of column vectors (initialized randomly) such that `v[i]` corresponds to $v^{(i)}$ as defined above.\n",
        "    * `x[3] = b_v`, a column vector (initialized with 0s) equal to $b_v$ as defined above.  \n",
        "* Then we alternate minimizations.\n",
        "* And report the results: the error between predicted scores and a held-out set of actual scores on the same users and items.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCsmTRm3_nA9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The ALS outer loop\n",
        "def mf_als(data_train, data_validate, k=2, lam=0.02, max_iter=100, verbose=False):\n",
        "    # size of the problem\n",
        "    n = max(d[0] for d in data_train)+1 # users\n",
        "    m = max(d[1] for d in data_train)+1 # items\n",
        "    # which entries are set in each row and column\n",
        "    us_from_v = [[] for i in range(m)]\n",
        "    vs_from_u = [[] for a in range(n)]\n",
        "    for (a, i, r) in data_train:\n",
        "        us_from_v[i].append((a, r))\n",
        "        vs_from_u[a].append((i, r))\n",
        "    # Initial guess at u, b_u, v, b_v\n",
        "    # Note that u and v are lists of column vectors (columns of U, V).\n",
        "    x = ([np.random.normal(1/k, size=(k,1)) for a in range(n)],\n",
        "          np.zeros(n),\n",
        "          [np.random.normal(1/k, size=(k,1)) for i in range(m)],\n",
        "          np.zeros(m))\n",
        "    # Alternation, modifies the contents of x\n",
        "    start_time = time.time()\n",
        "    for i in range(max_iter):\n",
        "        update_U(data_train, vs_from_u, x, k, lam)\n",
        "        update_V(data_train, us_from_v, x, k, lam)\n",
        "        if verbose:\n",
        "            print('train rmse', rmse(data_train, x), 'validate rmse', data_validate and rmse(data_validate, x))\n",
        "        if data_validate == None: # code is slower, print out progress\n",
        "            print(\"Iteration {} finished. Total Elapsed Time: {:.2f}\".format(i + 1, time.time() - start_time))\n",
        "    # The root mean square errors measured on validate set\n",
        "    if data_validate != None:\n",
        "        print('ALS result for k =', k, ': rmse train =', rmse(data_train, x), '; rmse validate =', rmse(data_validate, x))\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbO6NZnkAqdM",
        "colab_type": "text"
      },
      "source": [
        "The function `ridge_analytic(X,Y,lam)` is defined at the top of the file. Furthermore, the above code computes RMSE via the following function:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnCAMsfIAXay",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute the root mean square error\n",
        "def rmse(data, x):\n",
        "    error = 0.\n",
        "    for datum in data:\n",
        "        error += (datum[-1] - pred(datum, x))**2\n",
        "    return np.sqrt(error/len(data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3IBSKVLYyb9",
        "colab_type": "text"
      },
      "source": [
        "Here is an example of `vs_from_u` for the small data set given above:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxrGk0QwZTP1",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "vs_from_u = \\\n",
        "[[(0, 5), (1, 3), (3, 1)],\n",
        " [(0, 4), (3, 1)],\n",
        " [(0, 1), (1, 1), (3, 5)],\n",
        " [(0, 1), (3, 4)],\n",
        " [(1, 1), (2, 5), (3, 4)]]\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gt-zq_R7AYmP",
        "colab_type": "text"
      },
      "source": [
        "Now, the only part that's missing are the `update_U` and `update_V`\n",
        "procedures.  These are very similar, so we'll just do `update_U`,\n",
        "where we hold the $v^{(i)}$ constant and solve for the $u^{(a)}$.  We\n",
        "have seen above that each of the steps is solving a ridge regression\n",
        "problem, that is, finding a set of coefficients for a linear function,\n",
        "$(\\theta, \\theta_0)$, so as to minimize the mean sum of squared errors\n",
        "on data given by $(X,Y)$ (with regularization on the magnitude of\n",
        "$\\theta$)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2jNQBw7BZ6G",
        "colab_type": "text"
      },
      "source": [
        "Now, write the function `update_U(data, vs_from_u, x, k, lam)`\n",
        "\n",
        "* `data` is a list of `(a, i, r)` triples\n",
        "* `vs_from_u` is a list of lists as defined above\n",
        "* `x` is a list of parameters as defined above\n",
        "* `k` is an integer indicating the length of the individual u and v vectors\n",
        "* `lam` is the regularization parameter\n",
        "\n",
        "The function should update the entries in `x` corresponding to the `u` vectors and the `b_u` entries.  It should also return `x`, so the Tutor can check it.\n",
        "Note that if there are no ratings from a particular user, we don't want to update that user's entries.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pzy7mxNVBbKF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_U(data, vs_from_u, x, k, lam):\n",
        "    (u, b_u, v, b_v) = x\n",
        "    for a in range(len(u)):\n",
        "        if not vs_from_u[a]: continue\n",
        "        V = np.hstack([v[i] for (i, _) in vs_from_u[a]]).T\n",
        "        y = np.array([r-b_v[i] for (i, r) in vs_from_u[a]])\n",
        "        u[a], b_u[a] = ridge_analytic(V, y, lam)\n",
        "    return x\n",
        "# This is analogous"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SK2Mmh6sBjj-",
        "colab_type": "text"
      },
      "source": [
        "Here is a function to help test your code. It uses `ratings_small`, defined above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8yh1XDeCBKf",
        "colab_type": "code",
        "outputId": "b7883978-4721-4504-9ac7-9013e700464b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        }
      },
      "source": [
        "def update_U_test():\n",
        "  '''\n",
        "  This is a test function provided to help you debug your implementation\n",
        "  '''\n",
        "  k = 2\n",
        "  lam = 0.01\n",
        "  \n",
        "  vs_from_u = \\\n",
        "  \n",
        "  [[(0, 5), (1, 3), (3, 1)],\n",
        "   [(0, 4), (3, 1)],\n",
        "   [(0, 1), (1, 1), (3, 5)],\n",
        "   [(0, 1), (3, 4)],\n",
        "   [(1, 1), (2, 5), (3, 4)]]\n",
        "  \n",
        "  np.random.seed(0)\n",
        "  \n",
        "  first = []\n",
        "  for i in range(5):\n",
        "    first.append(np.random.rand(2, 1))\n",
        "  second = np.zeros((5,))\n",
        "  third = []\n",
        "  for i in range(5):\n",
        "    third.append(np.random.rand(2, 1))\n",
        "  fourth = np.zeros((5,))\n",
        "  x0 = (first, second, third, fourth)\n",
        "  \n",
        "  x_result = update_U(ratings_small, vs_from_u, x0, k, lam)\n",
        "  \n",
        "  assert np.all(np.isclose(x_result[0], np.array([[[4.048442188078757], [-2.5000082235465526]],\n",
        "                                                [[3.2715388359271054], [-1.2879317400952521]],\n",
        "                                                [[-6.237522315961142], [-2.9639103597721355]],\n",
        "                                                [[-3.2715388359271054], [1.2879317400952521]],\n",
        "                                                [[-4.87111151185168], [-1.761023196019822]] ])))\n",
        "  assert np.all(np.isclose(x_result[1].reshape(-1,), \n",
        "                           np.array([[3.043665230868208], [2.048616799474877], [7.462166369240114], [2.951383200525123], [5.487071919883842]]).reshape(-1,)))\n",
        "  assert np.all(np.isclose(x_result[2], np.array([[[0.7917250380826646], [0.5288949197529045]],\n",
        "                                       [[0.5680445610939323], [0.925596638292661]],\n",
        "                                       [[0.07103605819788694], [0.08712929970154071]],\n",
        "                                       [[0.02021839744032572], [0.832619845547938]],\n",
        "                                       [[0.7781567509498505], [0.8700121482468192]] ])))\n",
        "  assert np.all(np.isclose(x_result[3].reshape(-1,), np.array([[0.0], [0.0], [0.0], [0.0], [0.0]]).reshape(-1,)))\n",
        "  print(\"Test passed!\")\n",
        "  \n",
        "update_U_test()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-1e65ef9a1187>\"\u001b[0;36m, line \u001b[0;32m8\u001b[0m\n\u001b[0;31m    vs_from_u =\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFRxfWPHDRR_",
        "colab_type": "text"
      },
      "source": [
        "**NOTE: ** The following **does not** need to be written: it can be filled in by clicking View Answer in the update_U problem. It will be useful in part 4."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBXOkPRlDsPv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "def update_V(data, us_from_v, x, k, lam):\n",
        "    (u, b_u, v, b_v) = x\n",
        "    for i in range(len(v)):\n",
        "        if not us_from_v[i]: continue\n",
        "        V = np.hstack([u[a] for (a, _) in us_from_v[i]]).T\n",
        "        y = np.array([r-b_u[a] for (a, r) in us_from_v[i]])\n",
        "        v[i], b_v[i] = ridge_analytic(V, y, lam)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WXU4LYCCSAU",
        "colab_type": "text"
      },
      "source": [
        "### 3.2) Stochastic Gradient Descent (SGD)\n",
        "Alternatively, we can use Stochastic Gradient Descent directly on the objective function\n",
        "$$J(U, V) = \\frac{1}{2}\\sum_{(a, i) \\in D} (Y_{ai} - {u^{(a)}}\\cdot v^{(i)} - b_u^{(a)} - b_v^{(i)})^2 + \\frac{\\lambda}{2} \\sum_{a = 1}^n \\lVert u^{(a)} \\rVert^2 + \\frac{\\lambda}{2} \\sum_{i = 1}^m  \\lVert v^{(i)} \\rVert^2 $$\n",
        "Note, however, that this is not strictly a sum over the data.  The regularization terms are not inside the first summation and they cannot simply be moved inside, since we would end up penalizing the parameters unevenly depending on how many ratings there were for particular items by particular users.\n",
        "\n",
        "We can, however, define vectors of values $\\lambda_u^{(a)}$ and $\\lambda_v^{(i)}$ so that this is equivalent to the original objective:\n",
        "$$J(U, V) = \\frac{1}{2}\\sum_{(a, i) \\in D} \\left[(Y_{ai} - {u^{(a)}}\\cdot v^{(i)} - b_u^{(a)} - b_v^{(i)})^2 + \\lambda_u^{(a)}\\lVert u^{(a)} \\rVert^2 + \\lambda_v^{(i)} \\lVert v^{(i)} \\rVert^2\\right] $$\n",
        "\n",
        "In order to make this work out, we must have had:\n",
        "$$\\frac{\\lambda}{2} \\sum_{a = 1}^n \\lVert u^{(a)} \\rVert^2 = \\frac{1}{2}\\sum_{(a, i) \\in D} \\lambda_u^{(a)}\\lVert u^{(a)} \\rVert^2 $$\n",
        "matching the terms for each $a$ separately, we must have:\n",
        "$$ \\frac{\\lambda}{2} \\lVert u^{(a)} \\rVert^2 = \\sum_{i : (a, i) \\in D} \\frac{\\lambda_u^{(a)}}{2} \\lVert u^{(a)} \\rVert^2 = \\lvert \\{i : (a, i) \\in D\\} \\rvert \\frac{\\lambda_u^{(a)}}{2} \\lVert u^{(a)} \\rVert^2 $$\n",
        "Then, we must have:\n",
        "$$ \\lambda_u^{(a)} = \\frac{\\lambda}{\\lvert \\{i : (a, i) \\in D\\} \\rvert}$$\n",
        "Similarly,\n",
        "$$ \\lambda_v^{(i)} = \\frac{\\lambda}{\\lvert \\{a : (a, i) \\in D\\} \\rvert}$$\n",
        "\n",
        "\n",
        "This is now strictly a sum over the data. Note that the offset terms are not regularized.\n",
        "\n",
        "The \"outer loop\" of this approach is given here:\n",
        "\n",
        "* Define n and m from the data.\n",
        "* Define vectors $\\lambda_u$ and $\\lambda_v$.\n",
        "* Initialize the set of parameters (created and initialized as in ALS).  \n",
        "* Loop picking a random data entry `(a,i,r)` and taking a step down the gradient during each iteration.\n",
        "* And report the results: the error between predicted scores and a held-out set of actual scores on the same users and items."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwwpI-deCaQZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The SGD outer loop\n",
        "def mf_sgd(data_train, data_validate, step_size_fn, k=2, lam=0.02, max_iter=100, verbose=False):\n",
        "    # size of the problem\n",
        "    ndata = len(data_train)\n",
        "    n = max(d[0] for d in data_train)+1\n",
        "    m = max(d[1] for d in data_train)+1\n",
        "    # Distribute the lambda among the users and items\n",
        "    lam_uv = lam/counts(data_train,0), lam/counts(data_train,1)\n",
        "    # Initial guess at u, b_u, v, b_v (also b)\n",
        "    x = ([np.random.normal(1/k, size=(k,1)) for j in range(n)],\n",
        "         np.zeros(n),\n",
        "         [np.random.normal(1/k, size=(k,1)) for j in range(m)],\n",
        "         np.zeros(m))\n",
        "    di = int(max_iter/10.)\n",
        "    for i in range(max_iter):\n",
        "        if i%di == 0 and verbose:\n",
        "            print('i=', i, 'train rmse=', rmse(data_train, x),\n",
        "                  'validate rmse', data_validate and rmse(data_validate, x))\n",
        "        step = step_size_fn(i)\n",
        "        j = np.random.randint(ndata)            # pick data item\n",
        "        sgd_step(data_train[j], x, lam_uv, step) # modify x\n",
        "    print('SGD result for k =', k, ': rmse train =', rmse(data_train, x), '; rmse validate =', rmse(data_validate, x))\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dm9qTaclChU4",
        "colab_type": "text"
      },
      "source": [
        "The missing procedure is `sgd_step` which takes a gradient step using a\n",
        "noisy estimate of the gradient based on one point.  This function will\n",
        "update the relevant components of `x` corresponding to `u[a]`,\n",
        "`b_u[a]`, `v[i]` and `b_v[i]`.  Your function should modify the entries in `x`\n",
        "and also return `x` so that the Tutor can check it.\n",
        "\n",
        "You will need to derive the gradient for each of the variable\n",
        "components of `x`, including the offsets.  Refer back to section 1 for guidance, but\n",
        "remember the offsets and remember that the offset terms are not regularized.\n",
        "\n",
        "** WARNING: In numpy `x += y` and `x -= y` can produce different results from `x = x+y` and `x = x-y`.  For consistency in checking, please don't use `x += y` or `x -= y`! \n",
        "<a href=\"https://stackoverflow.com/questions/31987713/numpy-array-difference-between-a-x-vs-a-a-x\">Read here</a> **\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trIPrEQKCm-e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sgd_step(data, x, lam, step):\n",
        "    (a, i, r) = data\n",
        "    (u, b_u, v, b_v) = x\n",
        "    (lam_u, lam_v) = lam\n",
        "    # Your code here\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sn3Pck5qTls3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsQHchl2nyrt",
        "colab_type": "text"
      },
      "source": [
        "Here is a function to help test your code. It uses `ratings_small`, defined above. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdG9v_WdoELe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sgd_step_test():\n",
        "  '''\n",
        "  This is a test function provided to help you debug your implementation\n",
        "  '''\n",
        "  step = 0.025\n",
        "  lam =(np.array([ 0.00333333,  0.005,  0.00333333,  0.005,  0.00333333]), np.array([ 0.0025,  0.00333333,  0.01,  0.002]))\n",
        "  \n",
        "  np.random.seed(0)\n",
        "  \n",
        "  first = []\n",
        "  for i in range(5):\n",
        "    first.append(np.random.rand(2, 1))\n",
        "  second = np.zeros((5,))\n",
        "  third = []\n",
        "  for i in range(5):\n",
        "    third.append(np.random.rand(2, 1))\n",
        "  fourth = np.zeros((5,))\n",
        "  x0 = (first, second, third, fourth)\n",
        "  \n",
        "  x_result = sgd_step(ratings_small[3], x0, lam, step)\n",
        "    \n",
        "  assert np.all(np.isclose(x_result[0], np.array([[[0.5488135039273248], [0.7151893663724195]],\n",
        "                                                [[0.6667107015911342], [0.5875840438721468]],\n",
        "                                                [[0.4236547993389047], [0.6458941130666561]],\n",
        "                                                [[0.4375872112626925], [0.8917730007820798]],\n",
        "                                                [[0.9636627605010293], [0.3834415188257777]]])))\n",
        "  assert np.all(np.isclose(x_result[1].reshape(-1,), np.array([[0.0], [0.08086477989447478], [0.0], [0.0], [0.0]]).reshape(-1,)))\n",
        "  assert np.all(np.isclose(x_result[2], np.array([[[0.8404178830022684], [0.5729237224816648]],\n",
        "                                                [[0.5680445610939323], [0.925596638292661]],\n",
        "                                                [[0.07103605819788694], [0.08712929970154071]],\n",
        "                                                [[0.02021839744032572], [0.832619845547938]],\n",
        "                                                [[0.7781567509498505], [0.8700121482468192]]])))\n",
        "  assert np.all(np.isclose(x_result[3].reshape(-1,), np.array([[0.08086477989447478], [0.0], [0.0], [0.0], [0.0]]).reshape(-1,)))\n",
        "  print(\"Test passed!\")\n",
        "  \n",
        "sgd_step_test()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8kZ8OrXHPHL",
        "colab_type": "text"
      },
      "source": [
        "##4) MovieLens\n",
        "\n",
        "Now we're going to start working with some real-world data. There's a\n",
        "commonly used <a\n",
        "href=\"https://grouplens.org/datasets/movielens/\">dataset</a> of\n",
        "movie ratings, called the MovieLens Dataset.\n",
        "\n",
        "Below, we have included the following utility functions:\n",
        "\n",
        "* `load_ratings_data_small(path_data='ratings.csv')` returns\n",
        "  a list of ratings triples (a, i, r) for a subset of the data, to be\n",
        "  used in parameter tuning.\n",
        "\n",
        "* `load_ratings_data(path_data='ratings.csv')` returns the full list\n",
        "  of ratings triples.\n",
        "\n",
        "* `load_movies(path_movies='movies.csv')` returns two dictionaries.\n",
        "  The first maps movie indices to title strings and the second maps\n",
        "  movie indices to a list of genre strings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ua8q7jsAK-xG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_ratings_data_small(path_data='ratings.csv'):\n",
        "    \"\"\"\n",
        "    Returns two lists of triples (a, i, r) (training, validate)\n",
        "    \"\"\"\n",
        "    # we want to \"randomly\" sample but make it deterministic\n",
        "    def user_hash(uid):\n",
        "        return 71 * uid % 401\n",
        "    def user_movie_hash(uid, iid):\n",
        "        return (17 * uid + 43 * iid) % 61\n",
        "    data_train = []\n",
        "    data_validate = []\n",
        "    with open(path_data) as f_data:\n",
        "        for line in f_data:\n",
        "            (uid, iid, rating, timestamp) = line.strip().split(\",\")\n",
        "            h1 = user_hash(int(uid))\n",
        "            if h1 <= 40:\n",
        "                h2 = user_movie_hash(int(uid), int(iid))\n",
        "                if h2 <= 12:\n",
        "                    data_validate.append([int(uid), int(iid), float(rating)])\n",
        "                else:\n",
        "                    data_train.append([int(uid), int(iid), float(rating)])\n",
        "    print('Loading from', path_data,\n",
        "          'users_train', len(set(x[0] for x in data_train)),\n",
        "          'items_train', len(set(x[1] for x in data_train)),\n",
        "          'users_validate', len(set(x[0] for x in data_validate)),\n",
        "          'items_validate', len(set(x[1] for x in data_validate)))\n",
        "    return data_train, data_validate\n",
        "\n",
        "def load_ratings_data(path_data='ratings.csv'):\n",
        "    \"\"\"\n",
        "    Returns a list of triples (a, i, r)\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    with open(path_data) as f_data:\n",
        "        for line in f_data:\n",
        "            (uid, iid, rating, timestamp) = line.strip().split(\",\")\n",
        "            data.append([int(uid), int(iid), float(rating)])\n",
        "\n",
        "    print('Loading from', path_data,\n",
        "          'users', len(set(x[0] for x in data)),\n",
        "          'items', len(set(x[1] for x in data)))\n",
        "    return data\n",
        "\n",
        "def load_movies(path_movies='movies.csv'):\n",
        "    \"\"\"\n",
        "    Returns a dictionary mapping item_id to item_name and another dictionary\n",
        "    mapping item_id to a list of genres\n",
        "    \"\"\"\n",
        "    data = {}\n",
        "    genreMap = {}\n",
        "    with open(path_movies, encoding = \"utf8\") as f_data:\n",
        "        for line in f_data:\n",
        "            parts = line.strip().split(\",\")\n",
        "            item_id = int(parts[0])\n",
        "            item_name = \",\".join(parts[1:-1]) # file is poorly formatted\n",
        "            item_genres = parts[-1].split(\"|\")\n",
        "            data[item_id] = item_name\n",
        "            genreMap[item_id] = item_genres\n",
        "    return data, genreMap"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuXHXoNNLC4K",
        "colab_type": "text"
      },
      "source": [
        "You will need to use the above functions to answer the questions below.\n",
        "\n",
        "**NOTE: Our checkers will succeed on 95% of submissions stemming from\n",
        "correct implementations.  However, it is possible (but unlikely) for\n",
        "your implementation to be correct but your answer to be rejected due\n",
        "to getting an unlucky initialization.  If you are fairly sure that\n",
        "your code is correct, you should try re-training a new model and\n",
        "submitting answers from the new model.**\n",
        "\n",
        "If you haven't already, complete the implementations of `update_U`,\n",
        "`update_V`, and `sgd_step` (if you implemented it) above based on your solutions\n",
        "to the previous questions; the definition of `update_V` can be found via View Answer in\n",
        "the `update_U` problem.  We will limit ourselves to running ALS in\n",
        "this problem since it requires less parameter tuning than SGD does."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tng9UdUHf81",
        "colab_type": "text"
      },
      "source": [
        "###4.1) Recommendations\n",
        "\n",
        "In the following, we will want to be able to save models locally so as to avoid running als (which takes some time) too many times. In order to do this, we include the following helper functions:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_iEFmSpIamo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# After retrieving the output x from mf_als, you can use this function to save the output so\n",
        "# you don't have to re-train your model\n",
        "def save_model(x):\n",
        "    pickle.dump(x, open(\"ALSmodel\", \"wb\"))\n",
        "\n",
        "# After training and saving your model once, you can use this function to retrieve the previous model\n",
        "def load_model():\n",
        "    x = pickle.load(open(\"ALSmodel\", \"rb\"))\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INs64XAdIc-2",
        "colab_type": "text"
      },
      "source": [
        "Now, compute a model with ALS and save it to disk, by executing the following code:\n",
        "\n",
        "<b> Note: If you are running into errors when running `mf_als`, make sure that your code for `update_U` did not update a user's entries if that user had no ratings. </b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkfGyqHmHsyJ",
        "colab_type": "code",
        "outputId": "c9c4c1ad-fb7d-4848-ae74-877d7802b668",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "data = load_ratings_data()\n",
        "movies_dict, genres_dict = load_movies()\n",
        "model = mf_als(data, None, k=10, lam=1, max_iter=20)\n",
        "save_model(model)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading from ratings.csv users 13366 items 2000\n",
            "Iteration 1 finished. Total Elapsed Time: 8.21\n",
            "Iteration 2 finished. Total Elapsed Time: 16.31\n",
            "Iteration 3 finished. Total Elapsed Time: 24.51\n",
            "Iteration 4 finished. Total Elapsed Time: 32.60\n",
            "Iteration 5 finished. Total Elapsed Time: 40.77\n",
            "Iteration 6 finished. Total Elapsed Time: 48.95\n",
            "Iteration 7 finished. Total Elapsed Time: 57.04\n",
            "Iteration 8 finished. Total Elapsed Time: 65.18\n",
            "Iteration 9 finished. Total Elapsed Time: 73.27\n",
            "Iteration 10 finished. Total Elapsed Time: 81.36\n",
            "Iteration 11 finished. Total Elapsed Time: 89.47\n",
            "Iteration 12 finished. Total Elapsed Time: 97.64\n",
            "Iteration 13 finished. Total Elapsed Time: 105.74\n",
            "Iteration 14 finished. Total Elapsed Time: 113.90\n",
            "Iteration 15 finished. Total Elapsed Time: 122.01\n",
            "Iteration 16 finished. Total Elapsed Time: 130.13\n",
            "Iteration 17 finished. Total Elapsed Time: 138.27\n",
            "Iteration 18 finished. Total Elapsed Time: 146.53\n",
            "Iteration 19 finished. Total Elapsed Time: 154.65\n",
            "Iteration 20 finished. Total Elapsed Time: 162.75\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSbVkt-YTYBO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaSm5pbAHvK5",
        "colab_type": "text"
      },
      "source": [
        "This takes two minutes or so on a reasonably fast laptop. As alluded to before, <b>you only have to run `mf_als` once</b> -- henceforth, you can load the saved model via:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5coq_y-DIosy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = load_model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8zWX7LvIrHn",
        "colab_type": "text"
      },
      "source": [
        "Note that `load_model()` returns a tuple $(u, b_u, v, b_v)$ corresponding to the matrix factorization learned using the alternating least squares algorithm from above.\n",
        "\n",
        "We will assume this model for the rest of the questions below.\n",
        "\n",
        "We have introduced a \"synthetic\" user into the ratings file with id = 270894. Remember that we can access user data in `data`, which is a list of $(a, i, r)$ tuples\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQ6zZ5qrIzSr",
        "colab_type": "text"
      },
      "source": [
        "1) Write a piece of code to get relevant movie ratings in `data`, then look for the movies in `genres_dict`. Based on the movies that they've rated 5.0, what is this user's favorite genre? A list of all possible genres is given by the `genres` list below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZ1ez7IQI2H6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "genres = ['Western', 'Comedy', 'Children', 'Crime', 'Musical',\n",
        "          'Adventure', 'Drama', 'Horror', 'War', 'Documentary',\n",
        "          'Romance', 'Animation', 'Film-Noir', 'Sci-Fi', 'Mystery',\n",
        "          'Fantasy', 'IMAX', 'Action', 'Thriller']\n",
        "\n",
        "# Your code to find the user's favorite genre here:\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SJbluRnI813",
        "colab_type": "text"
      },
      "source": [
        "2) Write a piece of code to find the top 50 movies in `movies_dict` that are predicted for this user.  How many movies in this list match their favorite genre? Remember to remove movies the user has already seen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCPUnW6HJGKj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Your code to find how many movies match their favorite genre here\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLGXJQMkJKVg",
        "colab_type": "text"
      },
      "source": [
        "### 4.2) Similarity\n",
        "\n",
        "Typically, movie similarities are estimated based on similarity of\n",
        "genre or cast, or by human-generated recommendations.  It turns out that\n",
        "training a recommender system gives us, as a by-product, a new way of\n",
        "measuring similarity between movies.\n",
        "\n",
        "First, let's think about what movie similarity might mean.  One\n",
        "intuition is that two movies A and B are similar if lots of people\n",
        "like both of them.  However, this criterion can be misleading.  Movies\n",
        "with high average ratings will look similar since lots of people will tend to\n",
        "like them.  So we will refine our criterion to be: \"If users tend to\n",
        "like movies A and B more than what is average for the movie and for\n",
        "the user, then A and B are similar\".\n",
        "\n",
        "We can generalize the above criterion to be \"If users tend to rate movies A and B similarly relative to the average, then A and B are similar\". Our previous criterion\n",
        "only discussed the case where ratings are high, but this new criterion should hold for low \n",
        "ratings as well, for example. So in general, if movies A and B are similar, then for\n",
        "every user $a$ we should expect the dot product/angle (based on the answer to the previous question)\n",
        "between $u^{(a)}$ and $v^{(A)}, v^{(B)}$ to be very close. This in turn implies that the two vectors\n",
        "$v^{(A)}, v^{(B)}$ are either very close together in space or very close in angle, again\n",
        "depending on the answer to the previous question. This motivates us to use <a\n",
        "href=\"https://en.wikipedia.org/wiki/Cosine_similarity\">cosine\n",
        "similarity</a> to compute similarities between movies, being the standard way\n",
        "to measure the type of proximity we are looking for. For the rest of\n",
        "this homework the similarity between movies $A$ and $B$ will be\n",
        "computed as $\\frac{v^{(A)} \\cdot v^{(B)}}{\\|v^{(A)}\\| \\|v^{(B)}\\|}$.\n",
        "**Note that the similarity can be positive or negative.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pc0pEdqYKF1q",
        "colab_type": "text"
      },
      "source": [
        "2) Write a piece of code which identifies the movies in the dataset with\n",
        "the highest similarity to a given movie, using the formula for cosine\n",
        "similarity defined above. Then use it to:\n",
        "\n",
        "a) Find the 10 movies most similar to \"Star Wars: Episode IV - A New Hope (1977)\" (id 260).\n",
        "\n",
        "b) Find the 10 movies most similar to \"Star Wars: Episode I - The Phantom Menace (1999)\" (id 2628)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIKrrrHuKMEt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Code to identify movies with highest similarity here\n",
        "\n",
        "\n",
        "\n",
        "#Code to answer (a) and (b)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DHxeY8EKPCu",
        "colab_type": "text"
      },
      "source": [
        "Make sure that you also print the movie names (using `movies_dict`) so you can see that they\n",
        "make sense!  These results are fairly interesting; they would be quite\n",
        "clear to one who has seen the movies as the Star Wars prequels (Episodes\n",
        "I-III) and the originals (Episodes IV-VI) are very different movies!\n",
        "But it's interesting that our model can learn this difference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMbyORY6KmpY",
        "colab_type": "text"
      },
      "source": [
        "3) Now, we look at how similar movies within the same genre are. You can\n",
        "use `genres_dict` to help with this.\n",
        "\n",
        "a) For calibration, compute the average similarity between all pairs\n",
        "of movies.  **Remember not to compare a movie to itself.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiihalEzKqFQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Code to compute average similarity between all pairs of movies\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEy5Q8OdKs-o",
        "colab_type": "text"
      },
      "source": [
        "Now compute the average similarities across all pairs of movies within\n",
        "each genre. Remember that a list of all possible genres is given by\n",
        "the `genres` list defined at the top of your code file.\n",
        "\n",
        "b) Find the genre whose movies have the highest average similarity and the value of that average similarity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlIQ2camL-hI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Code to find the genre with the highest average similarity and its value\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLb9V0ilMGHK",
        "colab_type": "text"
      },
      "source": [
        "c) Find the genre whose movies have the lowest average similarity and the value of that average similarity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v59Td1uRMHKd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d6cea9b4-17f4-4809-c5d6-3dbed8211a5a"
      },
      "source": [
        "genres = ['Western', 'Comedy', 'Children', 'Crime', 'Musical',\n",
        "          'Adventure', 'Drama', 'Horror', 'War', 'Documentary',\n",
        "          'Romance', 'Animation', 'Film-Noir', 'Sci-Fi', 'Mystery',\n",
        "          'Fantasy', 'IMAX', 'Action', 'Thriller']\n",
        "\n",
        "data = load_ratings_data()\n",
        "movies_dict, genres_dict = load_movies()\n",
        "model = load_model()\n",
        "_, _, v, _ = model\n",
        "mov_genre=dict([(key, []) for key in genres])\n",
        "sim_genre=dict([(key, []) for key in genres])\n",
        "\n",
        "for k in movies_dict.keys():\n",
        "  for each_genre in genres:\n",
        "    if(each_genre in genres_dict[k]):\n",
        "      mov_genre[each_genre].append(k)\n",
        "    else:\n",
        "      continue\n",
        "      \n",
        "\n",
        "\n"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading from ratings.csv users 13366 items 2000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sUEvISlVUD9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "genres_no_com = ['Western', 'Children', 'Crime', 'Musical',\n",
        "          'Adventure', 'Drama', 'Horror', 'War', 'Documentary',\n",
        "          'Romance', 'Animation', 'Film-Noir', 'Sci-Fi', 'Mystery',\n",
        "          'Fantasy', 'IMAX', 'Action', 'Thriller']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfjaV_VZMKyH",
        "colab_type": "text"
      },
      "source": [
        "These results tell us that there is enormous variation in how similar\n",
        "the movies under most genres are.  But, note that the similarities are\n",
        "generally *positive* for movies in a genre.\n",
        "\n",
        "Next we look at the similarities across genres, that is, find the\n",
        "average similarities across pairs of movies, where each pair has\n",
        "movies from different genres.  Note that movies can belong to multiple\n",
        "genres.\n",
        "\n",
        "d) Which genre is most similar to Comedy?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRMmA0x06dpd",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4QE_6zCMPIm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Code to find the genre most similar to Comedy\n",
        "\n",
        "withoutcom=dict([(key, []) for key in genres_no_com])\n",
        "\n",
        "for comedy in mov_genre['Comedy']:\n",
        "  for e_genre in genres_no_com:\n",
        "    for first in mov_genre[e_genre]: \n",
        "      if comedy==first: continue\n",
        "      else:\n",
        "          sim = v[first].T.dot(v[comedy])/(np.linalg.norm(v[first])*np.linalg.norm(v[comedy]))\n",
        "          withoutcom[e_genre].append(sim)\n",
        "\n",
        "      \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "              "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8TzyMcnMWwx",
        "colab_type": "text"
      },
      "source": [
        "e) Which genre is least similar to Comedy?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Wwt6iy0MaEf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Code to find genre least similar to Comedy\n",
        "\n",
        "for i in genres_no_com:\n",
        "  withoutcom[i]=np.mean(withoutcom[i])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4wSGshPtyII",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "71c49d5d-991b-450c-d8b2-715902892105"
      },
      "source": [
        "print(withoutcom)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'Western': -0.017170245183230202, 'Children': 0.05481330156849425, 'Crime': -0.02743842139591829, 'Musical': 0.0425283186500804, 'Adventure': 0.007561867765702895, 'Drama': -0.024012776256183626, 'Horror': -0.036306417267960196, 'War': -0.05514376283719031, 'Documentary': 0.021438593768420326, 'Romance': 0.029912850047773904, 'Animation': 0.037122662163300374, 'Film-Noir': -0.059358060401422205, 'Sci-Fi': -0.01792009430666002, 'Mystery': -0.0380516599745772, 'Fantasy': 0.022952294071307597, 'IMAX': -0.022310805342424408, 'Action': -0.011130868930925329, 'Thriller': -0.03227587124803351}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgcwGfZNxX0f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "c6a7409f-2bf7-4b73-8b60-c9281586d45e"
      },
      "source": [
        "data = load_ratings_data()\n",
        "movies_dict, genres_dict = load_movies()\n",
        "model = load_model()\n",
        "_, _, v, _ = model\n",
        "comedies = [i for i, my_genres in genres_dict.items() if \"Comedy\" in my_genres]\n",
        "for genre in genres:\n",
        "    if genre == \"Comedy\": continue\n",
        "    movies = [i for i, my_genres in genres_dict.items() if genre in my_genres]\n",
        "    sims = []\n",
        "    for i in comedies:\n",
        "        for j in movies:\n",
        "            if i == j: continue\n",
        "            sim = v[i].T.dot(v[j])/(np.linalg.norm(v[i])*np.linalg.norm(v[j]))\n",
        "            sims.append(sim)\n",
        "    print(genre, np.mean(sims))"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading from ratings.csv users 13366 items 2000\n",
            "Western -0.017170245183230202\n",
            "Children 0.05481330156849425\n",
            "Crime -0.02743842139591829\n",
            "Musical 0.0425283186500804\n",
            "Adventure 0.007561867765702895\n",
            "Drama -0.024012776256183626\n",
            "Horror -0.036306417267960196\n",
            "War -0.05514376283719031\n",
            "Documentary 0.021438593768420326\n",
            "Romance 0.029912850047773904\n",
            "Animation 0.037122662163300374\n",
            "Film-Noir -0.059358060401422205\n",
            "Sci-Fi -0.01792009430666002\n",
            "Mystery -0.0380516599745772\n",
            "Fantasy 0.022952294071307597\n",
            "IMAX -0.022310805342424408\n",
            "Action -0.011130868930925329\n",
            "Thriller -0.03227587124803351\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QN0eZ70JMcsB",
        "colab_type": "text"
      },
      "source": [
        "Observe that this last similarity value is negative."
      ]
    }
  ]
}